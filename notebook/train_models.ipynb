{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from imutils import paths\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"C:\\\\Users\\\\T-GAMER\\\\Downloads\\\\base_dados\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths=list(paths.list_images(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T-GAMER\\miniconda3\\envs\\treinamento\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "\n",
    "for i in imagePaths:\n",
    "    label=i.split(os.path.sep)[-2]\n",
    "    labels.append(label)\n",
    "    image = load_img(i,target_size=(96,96))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype='float32')\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting of Data\n",
    "\n",
    "train_X,test_X,train_Y,test_Y = train_test_split(data,labels,test_size=0.20,random_state=10,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug=ImageDataGenerator(\n",
    "                       rotation_range=20,\n",
    "                       zoom_range=0.15,width_shift_range=0.2,\n",
    "                       height_shift_range=0.2,shear_range=0.15,\n",
    "                       horizontal_flip=True,\n",
    "                       vertical_flip=True,\n",
    "                       fill_mode='nearest'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "# Build Model BEST MODEL\n",
    "input_image = Input(shape=input_shape)\n",
    "# 1st Conv layer\n",
    "model = tf.keras.applications.MobileNetV3Small()\n",
    "model = Sequential([model,Flatten(),Dense(1024),Dense(64),Dropout(0.2),Dense(2, activation='softmax')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0005\n",
    "Epochs=100\n",
    "BS=32\n",
    "\n",
    "opt=Adam(learning_rate=lr,decay=lr/Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/102 [==============================] - 83s 773ms/step - loss: 0.3226 - accuracy: 0.8652 - val_loss: 0.5388 - val_accuracy: 0.8523\n",
      "Epoch 2/100\n",
      "102/102 [==============================] - 84s 826ms/step - loss: 0.1416 - accuracy: 0.9546 - val_loss: 0.4469 - val_accuracy: 0.8755\n",
      "Epoch 3/100\n",
      "102/102 [==============================] - 78s 763ms/step - loss: 0.0807 - accuracy: 0.9750 - val_loss: 0.5875 - val_accuracy: 0.8791\n",
      "Epoch 4/100\n",
      "102/102 [==============================] - 79s 777ms/step - loss: 0.0799 - accuracy: 0.9747 - val_loss: 0.3259 - val_accuracy: 0.9243\n",
      "Epoch 5/100\n",
      "102/102 [==============================] - 78s 763ms/step - loss: 0.0693 - accuracy: 0.9787 - val_loss: 0.2020 - val_accuracy: 0.9609\n",
      "Epoch 6/100\n",
      "102/102 [==============================] - 88s 860ms/step - loss: 0.0548 - accuracy: 0.9778 - val_loss: 0.1450 - val_accuracy: 0.9683\n",
      "Epoch 7/100\n",
      "102/102 [==============================] - 80s 785ms/step - loss: 0.0456 - accuracy: 0.9836 - val_loss: 0.0846 - val_accuracy: 0.9756\n",
      "Epoch 8/100\n",
      "102/102 [==============================] - 82s 799ms/step - loss: 0.0514 - accuracy: 0.9833 - val_loss: 0.1378 - val_accuracy: 0.9670\n",
      "Epoch 9/100\n",
      "102/102 [==============================] - 82s 807ms/step - loss: 0.0379 - accuracy: 0.9877 - val_loss: 0.1214 - val_accuracy: 0.9744\n",
      "Epoch 10/100\n",
      "102/102 [==============================] - 82s 806ms/step - loss: 0.0328 - accuracy: 0.9877 - val_loss: 0.0546 - val_accuracy: 0.9841\n",
      "Epoch 11/100\n",
      "102/102 [==============================] - 79s 769ms/step - loss: 0.0272 - accuracy: 0.9911 - val_loss: 0.0708 - val_accuracy: 0.9817\n",
      "Epoch 12/100\n",
      "102/102 [==============================] - 77s 759ms/step - loss: 0.0265 - accuracy: 0.9926 - val_loss: 0.0354 - val_accuracy: 0.9915\n",
      "Epoch 13/100\n",
      "102/102 [==============================] - 79s 777ms/step - loss: 0.0325 - accuracy: 0.9898 - val_loss: 0.0628 - val_accuracy: 0.9841\n",
      "Epoch 14/100\n",
      "102/102 [==============================] - 82s 800ms/step - loss: 0.0268 - accuracy: 0.9911 - val_loss: 0.0762 - val_accuracy: 0.9841\n",
      "Epoch 15/100\n",
      "102/102 [==============================] - 81s 797ms/step - loss: 0.0366 - accuracy: 0.9874 - val_loss: 0.0392 - val_accuracy: 0.9878\n",
      "Epoch 16/100\n",
      "102/102 [==============================] - 78s 760ms/step - loss: 0.0273 - accuracy: 0.9914 - val_loss: 0.0502 - val_accuracy: 0.9902\n",
      "Epoch 17/100\n",
      "102/102 [==============================] - 78s 762ms/step - loss: 0.0176 - accuracy: 0.9938 - val_loss: 0.0301 - val_accuracy: 0.9951\n",
      "Epoch 18/100\n",
      "102/102 [==============================] - 84s 823ms/step - loss: 0.0379 - accuracy: 0.9889 - val_loss: 0.0644 - val_accuracy: 0.9841\n",
      "Epoch 19/100\n",
      "102/102 [==============================] - 85s 833ms/step - loss: 0.0268 - accuracy: 0.9920 - val_loss: 0.0415 - val_accuracy: 0.9866\n",
      "Epoch 20/100\n",
      "102/102 [==============================] - 86s 845ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.1833 - val_accuracy: 0.9609\n",
      "Epoch 21/100\n",
      "102/102 [==============================] - 85s 835ms/step - loss: 0.0200 - accuracy: 0.9917 - val_loss: 0.0923 - val_accuracy: 0.9780\n",
      "Epoch 22/100\n",
      "102/102 [==============================] - 80s 788ms/step - loss: 0.0274 - accuracy: 0.9898 - val_loss: 0.1942 - val_accuracy: 0.9487\n",
      "Epoch 23/100\n",
      "102/102 [==============================] - 78s 764ms/step - loss: 0.0444 - accuracy: 0.9880 - val_loss: 0.0925 - val_accuracy: 0.9805\n",
      "Epoch 24/100\n",
      "102/102 [==============================] - 79s 776ms/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.0696 - val_accuracy: 0.9853\n",
      "Epoch 25/100\n",
      "102/102 [==============================] - 79s 770ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.0377 - val_accuracy: 0.9902\n",
      "Epoch 26/100\n",
      "102/102 [==============================] - 77s 759ms/step - loss: 0.0335 - accuracy: 0.9892 - val_loss: 0.0554 - val_accuracy: 0.9841\n",
      "Epoch 27/100\n",
      "102/102 [==============================] - 80s 779ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0740 - val_accuracy: 0.9829\n",
      "Epoch 28/100\n",
      "102/102 [==============================] - 85s 834ms/step - loss: 0.0185 - accuracy: 0.9932 - val_loss: 0.0604 - val_accuracy: 0.9902\n",
      "Epoch 29/100\n",
      "102/102 [==============================] - 80s 782ms/step - loss: 0.0157 - accuracy: 0.9951 - val_loss: 0.0431 - val_accuracy: 0.9902\n",
      "Epoch 30/100\n",
      "102/102 [==============================] - 83s 816ms/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0697 - val_accuracy: 0.9902\n",
      "Epoch 31/100\n",
      "102/102 [==============================] - 77s 759ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.0610 - val_accuracy: 0.9890\n",
      "Epoch 32/100\n",
      "102/102 [==============================] - 80s 782ms/step - loss: 0.0119 - accuracy: 0.9957 - val_loss: 0.0347 - val_accuracy: 0.9951\n",
      "Epoch 33/100\n",
      "102/102 [==============================] - 81s 791ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.0760 - val_accuracy: 0.9866\n",
      "Epoch 34/100\n",
      "102/102 [==============================] - 79s 773ms/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0648 - val_accuracy: 0.9890\n",
      "Epoch 35/100\n",
      "102/102 [==============================] - 78s 768ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0435 - val_accuracy: 0.9878\n",
      "Epoch 36/100\n",
      "102/102 [==============================] - 81s 793ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 0.0350 - val_accuracy: 0.9927\n",
      "Epoch 37/100\n",
      "102/102 [==============================] - 78s 762ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.0550 - val_accuracy: 0.9915\n",
      "Epoch 38/100\n",
      "102/102 [==============================] - 80s 786ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0911 - val_accuracy: 0.9805\n",
      "Epoch 39/100\n",
      "102/102 [==============================] - 78s 769ms/step - loss: 0.0135 - accuracy: 0.9951 - val_loss: 0.0340 - val_accuracy: 0.9890\n",
      "Epoch 40/100\n",
      "102/102 [==============================] - 77s 753ms/step - loss: 0.0156 - accuracy: 0.9944 - val_loss: 0.0317 - val_accuracy: 0.9915\n",
      "Epoch 41/100\n",
      "102/102 [==============================] - 80s 785ms/step - loss: 0.0156 - accuracy: 0.9944 - val_loss: 0.0239 - val_accuracy: 0.9939\n",
      "Epoch 42/100\n",
      "102/102 [==============================] - 81s 796ms/step - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.0285 - val_accuracy: 0.9927\n",
      "Epoch 43/100\n",
      "102/102 [==============================] - 79s 771ms/step - loss: 0.0055 - accuracy: 0.9978 - val_loss: 0.0135 - val_accuracy: 0.9963\n",
      "Epoch 44/100\n",
      "102/102 [==============================] - 79s 775ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.0135 - val_accuracy: 0.9963\n",
      "Epoch 45/100\n",
      "102/102 [==============================] - 82s 801ms/step - loss: 0.0062 - accuracy: 0.9978 - val_loss: 0.0282 - val_accuracy: 0.9915\n",
      "Epoch 46/100\n",
      "102/102 [==============================] - 86s 840ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.0138 - val_accuracy: 0.9976\n",
      "Epoch 47/100\n",
      "102/102 [==============================] - 82s 797ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0494 - val_accuracy: 0.9890\n",
      "Epoch 48/100\n",
      " 37/102 [=========>....................] - ETA: 50s - loss: 0.0188 - accuracy: 0.9941"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "aug.flow(train_X,train_Y,batch_size=BS),\n",
    "steps_per_epoch=len(train_X)//BS,\n",
    "validation_data=(test_X,test_Y),\n",
    "validation_steps=len(test_X)//BS,\n",
    "epochs=Epochs\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
