{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d46b5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from shutil import unpack_archive\n",
    "from collections import OrderedDict\n",
    "from keras_preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb90c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN (object):\n",
    "    def __init__(self, dataset_path, img_width, img_height, batch_size, epochs, num_classes, model_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.num_classes = num_classes\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def train(self):\n",
    "        # Data augmentation\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        self.read_clean_data()\n",
    "\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            self.dataset_path + \"output/multi_train\",\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        validation_generator = test_datagen.flow_from_directory(\n",
    "            self.dataset_path + \"output/multi_val/\",\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical')\n",
    "\n",
    "        testing_generator = test_datagen.flow_from_directory(\n",
    "            self.dataset_path + \"output/multi_test/\",\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical'            \n",
    "        )\n",
    "\n",
    "        # Model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(96, (7, 7), strides=(2, 2), padding='same', input_shape=(self.img_width, self.img_height, 3)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "        model.add(Conv2D(256, (1, 1), strides=(1, 1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(384, (3, 3), strides=(1, 1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "        model.add(Conv2D(384, (1, 1), strides=(1, 1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4096))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(4096))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        #modelo de treinamento reconhecimento de faces usando o modelo de treinamento do AlexNet\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        # Training\n",
    "        sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "        checkpointer = ModelCheckpoint(filepath=self.model_path, verbose=1, save_best_only=True)\n",
    "        model.fit_generator( train_generator, \n",
    "                            steps_per_epoch=train_generator.n // self.batch_size, \n",
    "                            epochs=self.epochs, \n",
    "                            validation_data=validation_generator, \n",
    "                            validation_steps=testing_generator.n // self.batch_size, \n",
    "                            callbacks=[checkpointer])\n",
    "\n",
    "        # Save model\n",
    "        model.save(self.model_path)\n",
    "\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        model = keras.models.load_model(self.model_path)\n",
    "        img = image.load_img(img_path, target_size=(self.img_width, self.img_height))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        #x = preprocess_input(x)\n",
    "        preds = model.predict(x)\n",
    "        return preds\n",
    "\n",
    "    def read_clean_data(self):\n",
    "        lfw_allnames = pd.read_csv(self.dataset_path + \"lfw_allnames.csv\")\n",
    "\n",
    "        image_paths = lfw_allnames.loc[lfw_allnames.index.repeat(lfw_allnames['images'])]\n",
    "        image_paths['image_path'] = 1 + image_paths.groupby('name').cumcount()\n",
    "        image_paths['image_path'] = image_paths.image_path.apply(lambda x: '{0:0>4}'.format(x))\n",
    "        image_paths['image_path'] = image_paths.name + \"/\" + image_paths.name + \"_\" + image_paths.image_path + \".jpg\"\n",
    "        image_paths = image_paths.drop(\"images\",axis=1)\n",
    "\n",
    "        multi_data = pd.concat([image_paths[image_paths.name==\"George_W_Bush\"].sample(75),\n",
    "                        image_paths[image_paths.name==\"Colin_Powell\"].sample(75),\n",
    "                        image_paths[image_paths.name==\"Tony_Blair\"].sample(75),\n",
    "                        image_paths[image_paths.name==\"Donald_Rumsfeld\"].sample(75),\n",
    "                        image_paths[image_paths.name==\"Gerhard_Schroeder\"].sample(75),\n",
    "                        image_paths[image_paths.name==\"Ariel_Sharon\"].sample(75)])\n",
    "\n",
    "        print(\"Multi_Data \",len(multi_data))\n",
    "\n",
    "        multi_train, multi_test = train_test_split(multi_data, test_size=0.3)\n",
    "        multi_train, multi_val = train_test_split(multi_train,test_size=0.3)\n",
    "\n",
    "        self.directory_mover(multi_train,\"multi_train/\")\n",
    "        self.directory_mover(multi_val,\"multi_val/\")\n",
    "        self.directory_mover(multi_test,\"multi_test/\")\n",
    "\n",
    "\n",
    "    def directory_mover(self,data,dir_name):\n",
    "        co = 0\n",
    "        for image in data.image_path:\n",
    "            # create top directory\n",
    "            if not os.path.exists(os.path.join(self.dataset_path + 'output/',dir_name)):\n",
    "                shutil.os.mkdir(os.path.join(self.dataset_path + 'output/',dir_name))\n",
    "\n",
    "            data_type = data[data['image_path'] == image]['name']\n",
    "            data_type = str(list(data_type)[0])\n",
    "            if not os.path.exists(os.path.join(self.dataset_path + 'output/',dir_name,data_type)):\n",
    "                shutil.os.mkdir(os.path.join(self.dataset_path + 'output/',dir_name,data_type))\n",
    "            path_from = os.path.join(self.dataset_path + 'lfw-deepfunneled/',image)\n",
    "            path_to = os.path.join(self.dataset_path + 'output/',dir_name,data_type)\n",
    "            # print(path_to)\n",
    "            shutil.copy(path_from, path_to)\n",
    "            # print('Moved {} to {}'.format(image,path_to))\n",
    "            co += 1\n",
    "\n",
    "        print('Moved {} images to {} folder.'.format(co,dir_name))\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        # Clean image used for testing\n",
    "        if \"multi_train\" in os.listdir(self.dataset_path + \"output/\"):\n",
    "            shutil.rmtree(self.dataset_path + \"output/multi_train\")\n",
    "        if \"multi_val\" in os.listdir(self.dataset_path + \"output/\"):\n",
    "            shutil.rmtree(self.dataset_path + \"output/multi_val\")\n",
    "        if \"multi_test\" in os.listdir(self.dataset_path + \"output/\"):\n",
    "            shutil.rmtree(self.dataset_path + \"output/multi_test\")\n",
    "\n",
    "    def get_stats(self):\n",
    "        multi_test_names = []\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        multi_test_set = test_datagen.flow_from_directory(\n",
    "            self.dataset_path + \"output/multi_test/\",\n",
    "            target_size=(self.img_width, self.img_height),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical'            \n",
    "        )\n",
    "\n",
    "        for i in range(len(multi_test_set.filenames)):\n",
    "            multi_test_names.append(multi_test_set.filenames[i])\n",
    "        for i in range(len(multi_test_names)):\n",
    "            multi_test_names[i] = multi_test_names[i].split(\"/\")[0]\n",
    "\n",
    "        multi_test_name_order = list(OrderedDict.fromkeys(multi_test_names))\n",
    "        for i in range(len(multi_test_name_order)):\n",
    "            multi_test_name_order[i] = multi_test_name_order[i].replace(\"\\\\\",\"/\")\n",
    "            \n",
    "        predictions_values = 0\n",
    "        predictions_len = []\n",
    "        for i in range(len(multi_test_name_order)):\n",
    "            prediction = self.predict(self.dataset_path + \"output/multi_test/\" + multi_test_name_order[i])\n",
    "            predictions_values += prediction\n",
    "            predictions_len += [i] * len(prediction)\n",
    "\n",
    "        \n",
    "        multi_predic_frame = pd.DataFrame(list(zip(predictions_values,predictions_len)), columns=['Predictions','Actual'])\n",
    "        stats = self.prec_acc(multi_predic_frame)\n",
    "        print(\"Precision: \" + str(stats[1]))\n",
    "        print(\"Recall: \" + str(stats[2]))\n",
    "        print(\"Classes: \" + multi_test_name_order)\n",
    "        \n",
    "    def prec_acc(self, df):\n",
    "        precision = []\n",
    "        accuracy = []\n",
    "        recall = []\n",
    "        for i in range(len(set(df.Predictions))):\n",
    "            tp = df[np.logical_and(df['Actual'] == i, df['Predictions'] == i)].shape[0]\n",
    "            tn = df[np.logical_and(df['Actual'] != i, df['Predictions'] != i)].shape[0]\n",
    "            fp = df[np.logical_and(df['Actual'] != i, df['Predictions'] == i)].shape[0]\n",
    "            fn = df[np.logical_and(df['Actual'] == i, df['Predictions'] != i)].shape[0]\n",
    "            total_preds = df.shape[0]\n",
    "            precision.append(tp/(tp + fp))\n",
    "            accuracy.append((tp + tn)/total_preds)\n",
    "            recall.append(tp/(tp + fn))\n",
    "        return(accuracy,precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "693f3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_test = CNN(\"../dataset/\",250,250,32,14,6,\"../dataset/output/model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e69713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1028 images belonging to 6 classes.\n",
      "1/1 [==============================] - 1s 579ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_test.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c210f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
